{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hvvts9Bwh3Dg",
        "outputId": "cc9a2470-21ed-4427-acd6-3a3d47433077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m584.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "00bc5c8acad444cab1907e712b5f283d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def find_all_json_files(root_dir):\n",
        "    json_files = []\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for file in filenames:\n",
        "            if file.endswith(\".json\"):\n",
        "                full_path = os.path.join(dirpath, file)\n",
        "                json_files.append(full_path)\n",
        "    return json_files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5nL682JTcgVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWvX13Z6bxH2",
        "outputId": "38be372e-e8b7-4d02-c9f4-e1884a0f9ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_22_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_23_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_23_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_24_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_24_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_28_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_28_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_29_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_29_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers/jan_22_shift_1_questions.json\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/question_papers\"\n",
        "json_files = find_all_json_files(directory_path)\n",
        "\n",
        "# Print the list\n",
        "for path in json_files:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def extract_questions_from_json_files(json_file_paths, output_dir=None):\n",
        "    for file_path in json_file_paths:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Extract only the desired fields\n",
        "            extracted = []\n",
        "            for obj in data:\n",
        "                new_obj = {\n",
        "                    \"question\": obj.get(\"question\"),\n",
        "                }\n",
        "                extracted.append(new_obj)\n",
        "\n",
        "            # Determine output path\n",
        "            original_path = Path(file_path)\n",
        "            output_path = (\n",
        "                Path(output_dir) / f\"only_questions_{original_path.name}\"\n",
        "                if output_dir else\n",
        "                original_path.parent / f\"only_questions_{original_path.name}\"\n",
        "            )\n",
        "\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "                json.dump(extracted, out_f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"✅ Saved: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "NLpGm46uckNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_questions_from_json_files(json_files, \"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhhdRazddIyb",
        "outputId": "5a9b7936-0c0b-492a-9c8c-75eaa23a4e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_22_shift_2_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_23_shift_1_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_23_shift_2_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_24_shift_1_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_24_shift_2_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_28_shift_1_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_28_shift_2_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_29_shift_1_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_29_shift_2_questions.json\n",
            "✅ Saved: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/only_questions_jan_22_shift_1_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQDwaLQ9ft_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def save_json_object(data, output_dir, filename):\n",
        "    \"\"\"\n",
        "    Save a JSON object (list of dicts) to a file in the specified directory.\n",
        "\n",
        "    Args:\n",
        "        data (list): JSON-serializable list of dictionaries.\n",
        "        output_dir (str): Path to the directory where file will be saved.\n",
        "        filename (str): Name of the output file (e.g. \"output.json\").\n",
        "\n",
        "    Returns:\n",
        "        str: Full path to the saved file.\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Full file path\n",
        "    output_path = Path(output_dir) / filename\n",
        "\n",
        "    # Save JSON with pretty formatting\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✅ Saved file to: {output_path}\")\n",
        "    return str(output_path)\n"
      ],
      "metadata": {
        "id": "Vk7Z08mNhj32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_json_object(json_data, \"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_response\", \"only_questions_jan_22_shift_2_questions.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "EeNHADQ9hk-U",
        "outputId": "0b9b603f-133d-443d-f1c2-ffe7272714f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1513878109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_response\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"only_questions_jan_22_shift_2_questions.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def merge_llm_outputs_by_question(base_dir, llm_dir):\n",
        "    \"\"\"\n",
        "    Merges LLM results from llm_dir into matching files in base_dir based on the question text.\n",
        "\n",
        "    Args:\n",
        "        base_dir (str): Path to the directory containing base files (with 'question', 'options', 'answer').\n",
        "        llm_dir (str): Path to the directory containing LLM outputs (with 'question', 'llm_response', etc.).\n",
        "    \"\"\"\n",
        "    base_files = sorted([f for f in os.listdir(base_dir) if f.endswith(\".json\")])\n",
        "    llm_files = sorted([f for f in os.listdir(llm_dir) if f.endswith(\".json\")])\n",
        "\n",
        "    if len(base_files) != len(llm_files):\n",
        "        print(\"⚠️ Number of files in both directories don't match.\")\n",
        "        return\n",
        "\n",
        "    for base_file, llm_file in zip(base_files, llm_files):\n",
        "        base_path = os.path.join(base_dir, base_file)\n",
        "        llm_path = os.path.join(llm_dir, llm_file)\n",
        "\n",
        "        with open(base_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            base_data = json.load(f)\n",
        "\n",
        "        with open(llm_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            llm_data = json.load(f)\n",
        "\n",
        "        # Create mapping from question to LLM info\n",
        "        llm_map = {obj[\"question\"]: obj for obj in llm_data}\n",
        "\n",
        "        updated_count = 0\n",
        "        for obj in base_data:\n",
        "            q = obj.get(\"question\")\n",
        "            if q in llm_map:\n",
        "                obj[\"llm_response\"] = llm_map[q].get(\"llm_response\")\n",
        "                obj[\"llm_answer\"] = llm_map[q].get(\"llm_answer\")\n",
        "                obj[\"llm_match\"] = llm_map[q].get(\"llm_match\")\n",
        "                updated_count += 1\n",
        "\n",
        "        # Save back\n",
        "        with open(base_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(base_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✅ Updated {base_file} with {updated_count} LLM responses from {llm_file}\")\n"
      ],
      "metadata": {
        "id": "Tz0wLHDHhlCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir=\"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions\"\n",
        "llm_dir=\"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_response\"\n",
        "\n",
        "merge_llm_outputs_by_question(base_dir, llm_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfL1W92PgaEZ",
        "outputId": "463908a5-2c06-426f-9d0c-2c863b114f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated jan_22_shift_1_questions.json with 23 LLM responses from only_questions_jan_22_shift_1_questions.json\n",
            "✅ Updated jan_22_shift_2_questions.json with 21 LLM responses from only_questions_jan_22_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n"
      ],
      "metadata": {
        "id": "gciYyUqghLB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JudgeResult(BaseModel):\n",
        "    llm_match: bool = Field(description=\"True if the predicted and correct answer are equivalent.\")\n",
        "    llm_judge_reasoning: str = Field(description=\"Why the answers are considered equivalent or not.\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=JudgeResult)\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are evaluating a math problem.\n",
        "\n",
        "Your task:\n",
        "- Compare Pridicted Answer to the true correct answer.\n",
        "- Determine if they are mathematically equivalent — even if the format is different (e.g., `1/2` vs `\\frac{{1}}{{2}}` is equal)\n",
        "\n",
        "Do **not** assume equivalence if:\n",
        "- Derivation or symbolic algebra is required\n",
        "- The expressions are not clearly identical in value\n",
        "\n",
        "If the answers clearly represent the same value, mark `llm_match = true`. Otherwise, `false`. Provide your resoning why you think true or false.\n",
        "\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Predicted Answer:\n",
        "{llm_answer}\n",
        "\n",
        "Correct Answer:\n",
        "{answer}\n",
        "\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "OA6VovLvhSNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_judge_answers(file_paths, gemini_api_key, model=\"gemini-2.0-flash\"):\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        google_api_key=gemini_api_key\n",
        "    )\n",
        "\n",
        "    for path in file_paths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            updated = False\n",
        "\n",
        "            for i, obj in enumerate(tqdm(data, desc=f\"Processing {Path(path).name}\")):\n",
        "                if obj.get(\"llm_match\") is not None:\n",
        "                    continue  # Skip already judged\n",
        "\n",
        "                question = obj.get(\"question\", \"\")\n",
        "                answer = obj.get(\"answer\", \"\")\n",
        "                llm_answer = obj.get(\"llm_answer\", \"\")\n",
        "                if not question or not answer or not llm_answer:\n",
        "                    print(f\"⚠️ Missing fields at index {i} — skipping\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    prompt = prompt_template.format(\n",
        "                        question=question,\n",
        "                        answer=answer,\n",
        "                        llm_answer=llm_answer,\n",
        "                        format_instructions=parser.get_format_instructions()\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Prompt formatting failed at index {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                retries = 0\n",
        "                max_retries = 5\n",
        "                while retries < max_retries:\n",
        "                    try:\n",
        "                        result = llm.invoke(prompt)\n",
        "                        print(f\"🔎 Raw response at index {i}: {result.content}\")\n",
        "                        parsed = parser.parse(result.content)\n",
        "\n",
        "                        obj[\"llm_match\"] = parsed.llm_match\n",
        "                        obj[\"llm_judge_reasoning\"] = parsed.llm_judge_reasoning\n",
        "                        updated = True\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
        "                            retries += 1\n",
        "                            print(f\"⚠️ Rate limit hit (retry {retries}/{max_retries}). Sleeping 60s...\")\n",
        "                            time.sleep(60)\n",
        "                        else:\n",
        "                            print(f\"❌ Parse/LLM error at index {i}: {e}\")\n",
        "                            obj[\"llm_match\"] = None\n",
        "                            obj[\"llm_judge_reasoning\"] = \"Parsing failed or malformed response.\"\n",
        "                            break\n",
        "\n",
        "            if updated:\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                    f.flush()\n",
        "                    os.fsync(f.fileno())\n",
        "                print(f\"✅ Updated file: {Path(path).name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process file {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "Gs2hohUch-n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions\"\n",
        "json_files = find_all_json_files(directory_path)\n",
        "json_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWi-I5KClCgL",
        "outputId": "c80f9e55-590b-41b8-d081-cdc531877d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/jan_22_shift_1_questions.json',\n",
              " '/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/gpt-2o-mini/gpt_questions/jan_22_shift_2_questions.json']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = \"\"\n",
        "\n",
        "llm_judge_answers(json_files,gemini_api_key, model=\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peoqBJ2UiAkE",
        "outputId": "9e27e7a1-81d1-4425-88ee-d83dac824400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_1_questions.json:   4%|▍         | 1/25 [00:01<00:40,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 0: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 5, which are mathematically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:   8%|▊         | 2/25 [00:02<00:25,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 1: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are identical: e²–1 and e² – 1.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  12%|█▏        | 3/25 [00:03<00:26,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 2: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 22, so they are equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  16%|█▌        | 4/25 [00:04<00:22,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 3: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 1, while the correct answer is 29. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  20%|██        | 5/25 [00:05<00:18,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 4: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 20, while the correct answer is 22π². These values are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  24%|██▍       | 6/25 [00:06<00:17,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 5: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 24 and the correct answer is 48. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  28%|██▊       | 7/25 [00:06<00:15,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 6: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 49 and the correct answer is 784. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  32%|███▏      | 8/25 [00:07<00:15,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 7: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer (1,2,3) and the correct answer (-14/3, 22/3, 3) are different points in 3D space. They do not represent the same location.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  36%|███▌      | 9/25 [00:08<00:13,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 8: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 1, while the correct answer is e^(8/5). These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  40%|████      | 10/25 [00:09<00:12,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 9: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 0, while the correct answer is 2/3. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  44%|████▍     | 11/25 [00:10<00:11,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 10: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are exactly the same numerical value.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  48%|████▊     | 12/25 [00:11<00:10,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 11: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer (√5/2)e^{−π/4} and the correct answer 3 – e are not numerically equivalent. Therefore, they do not match.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  52%|█████▏    | 13/25 [00:11<00:09,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 12: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 6, which are mathematically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  56%|█████▌    | 14/25 [00:12<00:09,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 13: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 15, which are mathematically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  60%|██████    | 15/25 [00:13<00:08,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 14: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 1, so they are equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  64%|██████▍   | 16/25 [00:14<00:07,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 15: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 2525, which are mathematically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 41\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5). Sleeping 60s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  68%|██████▊   | 17/25 [01:17<02:36, 19.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 16: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 31, which are mathematically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  72%|███████▏  | 18/25 [01:18<01:38, 14.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 17: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer (3π - 3√3)/2 and the correct answer 6π – 16 are not mathematically equivalent. (3π - 3√3)/2 is approximately 0.684, while 6π – 16 is approximately 2.849. Therefore, they do not represent the same value.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  76%|███████▌  | 19/25 [01:19<01:00, 10.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 18: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 14, so they are equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  80%|████████  | 20/25 [01:20<00:36,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 19: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are identical: 288/5.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  84%|████████▍ | 21/25 [01:21<00:21,  5.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 20: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 20, while the correct answer is 34. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  88%|████████▊ | 22/25 [01:22<00:12,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 21: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 30, while the correct answer is 2035. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  92%|█████████▏| 23/25 [01:23<00:06,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 22: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 20 and the correct answer is 34. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_1_questions.json:  96%|█████████▌| 24/25 [01:24<00:02,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 23: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 52 and the correct answer is 216. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_1_questions.json: 100%|██████████| 25/25 [01:25<00:00,  3.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 24: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 6√2, while the correct answer is 16. These values are not equal.\"}\n",
            "```\n",
            "✅ Updated file: jan_22_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_2_questions.json:   4%|▍         | 1/25 [00:01<00:24,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 0: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is -9.5 and the correct answer is 5. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:   8%|▊         | 2/25 [00:01<00:20,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 1: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 144, so they are equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  12%|█▏        | 3/25 [00:02<00:21,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 2: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 21√3, while the correct answer is (343√3)/8. These values are not equal. 21√3 ≈ 36.37 and (343√3)/8 ≈ 74.24. Therefore, the answers are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  16%|█▌        | 4/25 [00:03<00:20,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 3: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 4100, and the correct answer is 280. These values are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  20%|██        | 5/25 [00:05<00:21,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 4: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 1, while the correct answer is 5. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  24%|██▍       | 6/25 [00:05<00:18,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 5: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 264 and the correct answer is 136. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  28%|██▊       | 7/25 [00:06<00:16,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 6: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 0, and the correct answer is e. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 24\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5). Sleeping 60s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  32%|███▏      | 8/25 [01:10<05:53, 20.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 7: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer \\\"1, 1\\\" and the correct answer \\\"2 and 3\\\" are not equivalent. They represent different numbers of local maximum and minimum points.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  36%|███▌      | 9/25 [01:10<03:53, 14.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 8: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are exactly the same: 3√5.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  40%|████      | 10/25 [01:12<02:37, 10.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 9: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is e^{arctan(1/3)} (1 – 2 arctan(1/3)), while the correct answer is e^(π/6). These are not equivalent. arctan(1/3) is not equal to pi/6, and the predicted answer has an additional term (1 – 2 arctan(1/3)).\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  44%|████▍     | 11/25 [01:13<01:46,  7.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 10: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is ln((7 + 4√3)/2) and the correct answer is (eπ)/(6√3). These expressions are not obviously equivalent, and it's unlikely they are equal without significant derivation. Therefore, I mark them as not matching.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  48%|████▊     | 12/25 [01:14<01:11,  5.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 11: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 25, so they are equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  52%|█████▏    | 13/25 [01:15<00:49,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 12: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both 151, which are numerically equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  56%|█████▌    | 14/25 [01:15<00:34,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 13: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 20, while the correct answer is 16. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  60%|██████    | 15/25 [01:16<00:24,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 14: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 2, while the correct answer is 0. These are distinct numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  64%|██████▍   | 16/25 [01:17<00:17,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 15: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 4 and the correct answer is 8. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  68%|██████▊   | 17/25 [01:18<00:13,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 16: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 1/6 and the correct answer is 9/4. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  72%|███████▏  | 18/25 [01:19<00:09,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 17: ```json\n",
            "{\"llm_match\": true, \"llm_judge_reasoning\": \"The predicted answer and the correct answer are both equal to pi.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  76%|███████▌  | 19/25 [01:20<00:07,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 18: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is π, while the correct answer is π/2 + 1. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  80%|████████  | 20/25 [01:20<00:05,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 19: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 32/3 and the correct answer is 8/3. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  84%|████████▍ | 21/25 [01:21<00:04,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 20: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 16, while the correct answer is 27. These are different numerical values.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  88%|████████▊ | 22/25 [01:22<00:02,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 21: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 50, while the correct answer is 145. These values are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  92%|█████████▏| 23/25 [01:23<00:01,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 22: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 3, while the correct answer is 28. These are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 7\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5). Sleeping 60s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing jan_22_shift_2_questions.json:  96%|█████████▌| 24/25 [02:26<00:19, 19.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 23: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 1/2, while the correct answer is 465. These values are not equal.\"}\n",
            "```\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_2_questions.json: 100%|██████████| 25/25 [02:27<00:00,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Raw response at index 24: ```json\n",
            "{\"llm_match\": false, \"llm_judge_reasoning\": \"The predicted answer is 7, while the correct answer is 3. These are different numerical values, so they are not equivalent.\"}\n",
            "```\n",
            "✅ Updated file: jan_22_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "\n",
        "# Define the system prompt used in your template\n",
        "SYSTEM_PROMPT = \"Think step-by-step; put only the final answer inside \\\\boxed{}.\"\n",
        "\n",
        "# Initialize tiktoken encoding (we'll assume GPT-3.5 / GPT-4 tokenizer for now)\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def add_token_counts_to_json_tiktoken(json_paths: list[str]):\n",
        "    for path in json_paths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            updated = False\n",
        "            for obj in tqdm(data, desc=f\"Processing {os.path.basename(path)}\"):\n",
        "                question = obj.get(\"question\", \"\")\n",
        "                response = obj.get(\"llm_response\", \"\")\n",
        "\n",
        "                # Prompt format with system message\n",
        "                prompt = (\n",
        "                    f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
        "                    f\"<|im_start|>user\\n{question}<|im_end|>\"\n",
        "                )\n",
        "\n",
        "                # Token counts\n",
        "                input_token_count = count_tokens(question)\n",
        "                prompt_token_count = count_tokens(prompt)\n",
        "                output_token_count = count_tokens(response)\n",
        "                total_token_count = prompt_token_count + output_token_count\n",
        "\n",
        "                obj[\"input_token_count\"] = input_token_count\n",
        "                obj[\"prompt_token_count\"] = prompt_token_count\n",
        "                obj[\"output_token_count\"] = output_token_count\n",
        "                obj[\"total_token_count\"] = total_token_count\n",
        "\n",
        "                updated = True\n",
        "\n",
        "            if updated:\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                    f.flush()\n",
        "                    os.fsync(f.fileno())\n",
        "                print(f\"✅ Token counts added to: {os.path.basename(path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "DAp_FpdxlV5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_token_counts_to_json_tiktoken(json_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aXOy6YqrFob",
        "outputId": "e6198c48-0fa2-408a-cc74-f7ffa882e077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_1_questions.json: 100%|██████████| 25/25 [00:00<00:00, 2824.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: jan_22_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jan_22_shift_2_questions.json: 100%|██████████| 25/25 [00:00<00:00, 2694.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: jan_22_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Answer"
      ],
      "metadata": {
        "id": "WGHpP7jIrU6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extended_summary_per_file(json_files):\n",
        "    file_results = []\n",
        "\n",
        "    for path in json_files:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        total_questions = len(data)\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        incorrect = 0\n",
        "        not_evaluated = 0\n",
        "        llm_judge_count = 0\n",
        "\n",
        "        input_tokens, output_tokens, prompt_tokens = [], [], []\n",
        "\n",
        "        for obj in data:\n",
        "            matched = obj.get(\"llm_match\")\n",
        "\n",
        "            # Token counts\n",
        "            if obj.get(\"input_token_count\") is not None:\n",
        "                input_tokens.append(obj[\"input_token_count\"])\n",
        "            if obj.get(\"output_token_count\") is not None:\n",
        "                output_tokens.append(obj[\"output_token_count\"])\n",
        "            if obj.get(\"prompt_token_count\") is not None:\n",
        "                prompt_tokens.append(obj[\"prompt_token_count\"])\n",
        "\n",
        "\n",
        "            if matched is True:\n",
        "                correct += 1\n",
        "            elif matched is False:\n",
        "                incorrect += 1\n",
        "            else:\n",
        "                not_evaluated += 1\n",
        "\n",
        "        total = correct + incorrect\n",
        "\n",
        "        def token_stats(lst):\n",
        "            return {\n",
        "                \"avg\": round(sum(lst) / len(lst), 2) if lst else 0,\n",
        "                \"min\": min(lst) if lst else 0,\n",
        "                \"max\": max(lst) if lst else 0\n",
        "            }\n",
        "\n",
        "        input_stats = token_stats(input_tokens)\n",
        "        output_stats = token_stats(output_tokens)\n",
        "        prompt_stats = token_stats(prompt_tokens)\n",
        "\n",
        "        file_results.append({\n",
        "            \"file\": Path(path).name,\n",
        "            \"total_questions\": total_questions,\n",
        "            \"evaluated\": total,\n",
        "            \"correct\": correct,\n",
        "            \"incorrect\": incorrect,\n",
        "            \"not_evaluated\": not_evaluated,\n",
        "            \"% evaluated\": round((total / total_questions) * 100, 2) if total_questions else 0,\n",
        "            \"llm_judged\": llm_judge_count,\n",
        "            \"correct_pct\": round((correct / total) * 100, 2) if total else 0,\n",
        "            \"incorrect_pct\": round((incorrect / total) * 100, 2) if total else 0,\n",
        "\n",
        "            # Token stats\n",
        "            \"input_avg_tokens\": input_stats[\"avg\"],\n",
        "            \"input_min_tokens\": input_stats[\"min\"],\n",
        "            \"input_max_tokens\": input_stats[\"max\"],\n",
        "\n",
        "            \"output_avg_tokens\": output_stats[\"avg\"],\n",
        "            \"output_min_tokens\": output_stats[\"min\"],\n",
        "            \"output_max_tokens\": output_stats[\"max\"],\n",
        "\n",
        "            \"prompt_avg_tokens\": prompt_stats[\"avg\"],\n",
        "            \"prompt_min_tokens\": prompt_stats[\"min\"],\n",
        "            \"prompt_max_tokens\": prompt_stats[\"max\"],\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(file_results)\n",
        "\n",
        "\n",
        "def grand_total_extended_summary(df: pd.DataFrame):\n",
        "    total = df[\"evaluated\"].sum()\n",
        "    correct = df[\"correct\"].sum()\n",
        "    incorrect = df[\"incorrect\"].sum()\n",
        "    not_evaluated = df[\"not_evaluated\"].sum()\n",
        "    llm_judged_total = df[\"llm_judged\"].sum()\n",
        "\n",
        "    summary = {\n",
        "        \"total_evaluated\": total,\n",
        "        \"correct\": correct,\n",
        "        \"incorrect\": incorrect,\n",
        "        \"not_evaluated\": not_evaluated,\n",
        "        \"llm_judged\": llm_judged_total,\n",
        "        \"correct_pct\": round((correct / total) * 100, 2) if total else 0,\n",
        "        \"incorrect_pct\": round((incorrect / total) * 100, 2) if total else 0,\n",
        "\n",
        "        # Token stats from all files\n",
        "        \"input_avg_tokens\": round(df[\"input_avg_tokens\"].mean(), 2),\n",
        "        \"input_min_tokens\": df[\"input_min_tokens\"].min(),\n",
        "        \"input_max_tokens\": df[\"input_max_tokens\"].max(),\n",
        "\n",
        "        \"output_avg_tokens\": round(df[\"output_avg_tokens\"].mean(), 2),\n",
        "        \"output_min_tokens\": df[\"output_min_tokens\"].min(),\n",
        "        \"output_max_tokens\": df[\"output_max_tokens\"].max(),\n",
        "\n",
        "        \"prompt_avg_tokens\": round(df[\"prompt_avg_tokens\"].mean(), 2),\n",
        "        \"prompt_min_tokens\": df[\"prompt_min_tokens\"].min(),\n",
        "        \"prompt_max_tokens\": df[\"prompt_max_tokens\"].max(),\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame([summary])\n"
      ],
      "metadata": {
        "id": "rCbqbqHbrMoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = extended_summary_per_file(json_files)\n",
        "df.to_csv(\"final_score_card.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "u4va62S1ryeU",
        "outputId": "4bffc163-9acd-4283-aeff-1ff89522dd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            file  total_questions  evaluated  correct  \\\n",
              "0  jan_22_shift_1_questions.json               25         25       11   \n",
              "1  jan_22_shift_2_questions.json               25         25        5   \n",
              "\n",
              "   incorrect  not_evaluated  % evaluated  llm_judged  correct_pct  \\\n",
              "0         14              0        100.0           0         44.0   \n",
              "1         20              0        100.0           0         20.0   \n",
              "\n",
              "   incorrect_pct  input_avg_tokens  input_min_tokens  input_max_tokens  \\\n",
              "0           56.0             57.88                19               123   \n",
              "1           80.0             62.60                32                94   \n",
              "\n",
              "   output_avg_tokens  output_min_tokens  output_max_tokens  prompt_avg_tokens  \\\n",
              "0             162.68                 13                530              98.88   \n",
              "1             237.12                128                434             103.60   \n",
              "\n",
              "   prompt_min_tokens  prompt_max_tokens  \n",
              "0                 60                164  \n",
              "1                 73                135  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fec31b53-88e7-4f2f-b376-25e092a7c780\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>total_questions</th>\n",
              "      <th>evaluated</th>\n",
              "      <th>correct</th>\n",
              "      <th>incorrect</th>\n",
              "      <th>not_evaluated</th>\n",
              "      <th>% evaluated</th>\n",
              "      <th>llm_judged</th>\n",
              "      <th>correct_pct</th>\n",
              "      <th>incorrect_pct</th>\n",
              "      <th>input_avg_tokens</th>\n",
              "      <th>input_min_tokens</th>\n",
              "      <th>input_max_tokens</th>\n",
              "      <th>output_avg_tokens</th>\n",
              "      <th>output_min_tokens</th>\n",
              "      <th>output_max_tokens</th>\n",
              "      <th>prompt_avg_tokens</th>\n",
              "      <th>prompt_min_tokens</th>\n",
              "      <th>prompt_max_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jan_22_shift_1_questions.json</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>57.88</td>\n",
              "      <td>19</td>\n",
              "      <td>123</td>\n",
              "      <td>162.68</td>\n",
              "      <td>13</td>\n",
              "      <td>530</td>\n",
              "      <td>98.88</td>\n",
              "      <td>60</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jan_22_shift_2_questions.json</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>62.60</td>\n",
              "      <td>32</td>\n",
              "      <td>94</td>\n",
              "      <td>237.12</td>\n",
              "      <td>128</td>\n",
              "      <td>434</td>\n",
              "      <td>103.60</td>\n",
              "      <td>73</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fec31b53-88e7-4f2f-b376-25e092a7c780')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fec31b53-88e7-4f2f-b376-25e092a7c780 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fec31b53-88e7-4f2f-b376-25e092a7c780');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-df7ba399-a3c3-4513-b481-e52ec909d031\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df7ba399-a3c3-4513-b481-e52ec909d031')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-df7ba399-a3c3-4513-b481-e52ec909d031 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_962462db-5f0a-44cd-a9bd-6ad4f17c1a2e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_962462db-5f0a-44cd-a9bd-6ad4f17c1a2e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"jan_22_shift_2_questions.json\",\n          \"jan_22_shift_1_questions.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_questions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 25,\n        \"max\": 25,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 25,\n        \"max\": 25,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 5,\n        \"max\": 11,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 14,\n        \"max\": 20,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"not_evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"% evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 100.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"llm_judged\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.97056274847714,\n        \"min\": 20.0,\n        \"max\": 44.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          20.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.97056274847714,\n        \"min\": 56.0,\n        \"max\": 80.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3375440072005036,\n        \"min\": 57.88,\n        \"max\": 62.6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          62.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 19,\n        \"max\": 32,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 94,\n        \"max\": 123,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          94\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52.637028791526596,\n        \"min\": 162.68,\n        \"max\": 237.12,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          237.12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81,\n        \"min\": 13,\n        \"max\": 128,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67,\n        \"min\": 434,\n        \"max\": 530,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          434\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3375440072005036,\n        \"min\": 98.88,\n        \"max\": 103.6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          103.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 60,\n        \"max\": 73,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 135,\n        \"max\": 164,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcFxtSKPsF9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grand_total_extended_summary(df).to_csv(\"overall_score_card.csv\")"
      ],
      "metadata": {
        "id": "OjfXKh1Yr06h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qjNHNAor-jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}