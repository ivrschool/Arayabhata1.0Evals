{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGIZg8Q57SBi",
        "outputId": "2eecf3e6-1660-4836-bb75-9477ebeea7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gn2t_yHa78Rc",
        "outputId": "e74de6c2-38b6-4c9d-cd7d-9909acd1421c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cd9bf2bd9dbe4d779f12d5906d510bec"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMNmBp59FVXz",
        "outputId": "3ea3feab-4ad7-4cec-8e58-aa483fe536c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3s3x98B5jJN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "# # Load environment variables from .env file\n",
        "# load_dotenv()\n",
        "\n",
        "# # Fetch Gemini API key\n",
        "# gemini_api_key = \"\"\n",
        "\n",
        "# Confirm that the key was loaded (safe to print None for debugging, not actual key)\n",
        "# gemini_api_key is not None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# from dotenv import load_dotenv\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "rfTmk2wV75NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### clean the model responses as per the Aryabhata instruction on huggingface"
      ],
      "metadata": {
        "id": "HlrJrFrQQtYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def find_all_json_files(root_dir):\n",
        "    json_files = []\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for file in filenames:\n",
        "            if file.endswith(\".json\"):\n",
        "                full_path = os.path.join(dirpath, file)\n",
        "                json_files.append(full_path)\n",
        "    return json_files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KUGo9IhZQ3ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "directory_path = \"/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math\"\n",
        "json_files = find_all_json_files(directory_path)\n",
        "\n",
        "# Print the list\n",
        "for path in json_files:\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuUeFCYxRLd4",
        "outputId": "22ff7138-e5e8-4586-b2e0-f2a598650e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def count_json_objects_per_file(file_paths):\n",
        "    counts = {}\n",
        "    for path in file_paths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list):\n",
        "                    counts[path] = len(data)\n",
        "                else:\n",
        "                    counts[path] = 1  # It's a single JSON object\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {path}: {e}\")\n",
        "            counts[path] = 0\n",
        "    return counts\n",
        "\n",
        "\n",
        "file_paths = json_files\n",
        "counts = count_json_objects_per_file(file_paths)\n",
        "counts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGon0lQ2RLgN",
        "outputId": "e0642c73-5370-420f-de8f-79de552747d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_1_questions.json': 25,\n",
              " '/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_1_questions.json': 25,\n",
              " '/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json': 25,\n",
              " '/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_2_questions.json': 25}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import os\n",
        "\n",
        "# def rename_key_in_json_files(file_paths, old_key=\"aryabhata_response_Wquestion\", new_key=\"qwen-2.5-math_response_Wquestion\"):\n",
        "#     for path in file_paths:\n",
        "#         try:\n",
        "#             with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#                 data = json.load(f)\n",
        "\n",
        "#             modified = False\n",
        "#             for obj in data:\n",
        "#                 if old_key in obj:\n",
        "#                     obj[new_key] = obj.pop(old_key)\n",
        "#                     modified = True\n",
        "\n",
        "#             if modified:\n",
        "#                 with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "#                     json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "#                     f.flush()\n",
        "#                     os.fsync(f.fileno())\n",
        "#                 print(f\"✅ Updated {os.path.basename(path)}\")\n",
        "#             else:\n",
        "#                 print(f\"ℹ️ No change needed in {os.path.basename(path)}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"❌ Error processing {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "uEFoG8RZpdtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename_key_in_json_files(json_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mF92u1lpeyB",
        "outputId": "3919f153-0504-4b87-8f1c-bc4ff4064776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Updated pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Updated pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n",
            "✅ Updated pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print total questions and answers:\n",
        "count = 0\n",
        "for k, v in counts.items():\n",
        "    count += v\n",
        "\n",
        "print(\"Total number of JSON objects across all files:\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp88dKDiR5m9",
        "outputId": "11c8f326-2217-4889-c951-6c65f305c6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of JSON objects across all files: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_strings = [\"<|im_end|>\", \"<|end|>\", \"<im_start|>\", \"⁠```python\\n\", \"⁠<|im_start|>\", \"]}}]}}]\"]\n",
        "\n",
        "def strip_bad_tokens(s, stop_strings):\n",
        "    for suffix in stop_strings:\n",
        "        if s.endswith(suffix):\n",
        "            return s[:-len(suffix)]\n",
        "    return s"
      ],
      "metadata": {
        "id": "hpq8yhdjSUzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_update_json_files(json_file_paths, key_tag):\n",
        "    for file_path in json_file_paths:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for obj in data:\n",
        "                response = obj.get(\"qwen-2.5-math_response_Wquestion\", \"\")\n",
        "                cleaned = strip_bad_tokens(response, stop_strings)\n",
        "                obj[key_tag] = cleaned\n",
        "\n",
        "            # Save updated file\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Updated: {file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "Sq7_X4TgSdOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_and_update_json_files(json_files, \"qwen-2.5-math_response_Wquestion_cleaned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN2sNLuwTlq_",
        "outputId": "2af912f4-e44f-4f16-9226-63d00d32c683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import re\n",
        "\n",
        "def extract_assistant_reply(text):\n",
        "    match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return text.strip()  # fallback if assistant block not found\n"
      ],
      "metadata": {
        "id": "C-xlBPqvUDfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract assistant response only"
      ],
      "metadata": {
        "id": "vwCsvM5lVo2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_assistant_reply(text):\n",
        "    match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\", text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return text.strip()  # fallback if assistant block not found\n"
      ],
      "metadata": {
        "id": "AWdyIzFyV3Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_update_json_files_assistant_response(json_file_paths, key_tag):\n",
        "    for file_path in json_file_paths:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for obj in data:\n",
        "                response = obj.get(\"qwen-2.5-math_response_Wquestion\", \"\")\n",
        "                cleaned = extract_assistant_reply(response)\n",
        "                obj[key_tag] = cleaned\n",
        "\n",
        "            # Save updated file\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Updated: {file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "7T97BZe9UCIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_and_update_json_files_assistant_response(json_files, \"qwen-2.5-math_response_Wquestion_cleaned_ar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHBsMugsUALV",
        "outputId": "8906fade-9f9e-43e8-f5f4-0bd7fa24e07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### display the response in right formate"
      ],
      "metadata": {
        "id": "hm6COqWIW8AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def show_response_by_index(json_path, index, key=\"qwen-2.5-math_response_Wquestion_cleaned_ar\"):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if index < 0 or index >= len(data):\n",
        "            print(f\"Index {index} out of range. File contains {len(data)} entries.\")\n",
        "            return\n",
        "\n",
        "        entry = data[index]\n",
        "        question = entry.get(\"question\", \"No question found.\")\n",
        "        options = entry.get(\"options\", [])\n",
        "        response = entry.get(key, \"No response found.\")\n",
        "\n",
        "        opt_str = '\\n'.join([f\"{chr(65+i)}. {opt}\" for i, opt in enumerate(options)])\n",
        "\n",
        "        display(Markdown(f\"### Question {index + 1}\\n\\n**Q:** {question}\"))\n",
        "        if options:\n",
        "            display(Markdown(f\"**Options:**\\n\\n{opt_str}\"))\n",
        "        display(Markdown(f\"---\\n\\n### Model Response\\n\\n{response}\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "d-MK6d3fYYeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_response_by_index(json_files[0], 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "WMle_G0nXAjj",
        "outputId": "7af2dbaf-029b-4881-db05-d2b7e571a316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question 1\n\n**Q:** The number of non-empty equivalence relations on the set {1,2,3} is:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Options:**\n\nA. 6\nB. 7\nC. 5\nD. 4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---\n\n### Model Response\n\nsystem\nThink step-by-step; put only the final answer inside \\boxed{}.\nuser\nThe number of non-empty equivalence relations on the set {1,2,3} is:\nassistant\nThe number of non-empty equivalence relations on the set {1,2,3} is 5."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_response_by_index(json_files[3], 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "aUj2VmMoZkIz",
        "outputId": "c363811c-dc9d-47c4-d397-bf953d8ad5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question 11\n\n**Q:** If ∫ (x⁻¹(2x^2/3 + 2sin⁻¹x + sin x + x)/(1–x)√(1–x²)) dx = g(x)+C, where C is the constant of integration, then g(1/2) equals:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Options:**\n\nA. (eπ)/(6√2)\nB. (eπ)/(4√2)\nC. (eπ)/(6√3)\nD. (eπ)/(4√3)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---\n\n### Model Response\n\nsystem\nThink step-by-step; put only the final answer inside \\boxed{}.\nuser\nIf ∫ (x⁻¹(2x^2/3 + 2sin⁻¹x + sin x + x)/(1–x)√(1–x²)) dx = g(x)+C, where C is the constant of integration, then g(1/2) equals:\nassistant\nIf ∫ (x⁻¹(2x^2/3 + 2sin⁻¹x + sin x + x)/(1–x)√(1–x²)) dx = g(x)+C, where C is the constant of integration, then g(1/2) equals: 1/3 + 2sin⁻¹(1/2) + sin(1/2) + 1/2"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extact final answers from the responses and update the json files"
      ],
      "metadata": {
        "id": "ZuhOnG8YabZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_boxed_answer_from_json(json_path, index, key=\"qwen-2.5-math_response_Wquestion_cleaned_ar\"):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if index < 0 or index >= len(data):\n",
        "            print(f\"Index {index} out of range. File contains {len(data)} entries.\")\n",
        "            return None\n",
        "\n",
        "        response = data[index].get(key, \"\")\n",
        "        answer = data[index].get(\"answer\", \"\")\n",
        "\n",
        "        # Match \\boxed{ ... } with nested content like \\frac{1}{3}\n",
        "        match = re.search(r\"\\\\boxed\\{((?:[^{}]|\\{[^{}]*\\})+)\\}\", response)\n",
        "\n",
        "        if match:\n",
        "            return match.group(1).strip(), answer\n",
        "        else:\n",
        "            print(\"No \\\\boxed{} answer found in the response.\")\n",
        "            return None, answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "JXgF8gldbYfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_ans, ans  = extract_boxed_answer_from_json(json_files[0], 0, \"qwen-2.5-math_response_Wquestion_cleaned\")\n",
        "print(\"Extracted answer:\", ex_ans, \"real_answer: \", ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toJobzQhbZCx",
        "outputId": "458147ec-9472-43c3-ee5a-ad4845caac23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_response_by_index(json_files[1], 0)\n",
        "ex_ans, ans  = extract_boxed_answer_from_json(json_files[1], 0)\n",
        "print(\"Extracted answer:\", ex_ans, \"real_answer: \", ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "gkGj_9FUbZFI",
        "outputId": "22f7cd34-7ca8-4c24-c4da-370af2b8c3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question 1\n\n**Q:** The value of ∫(logₑx + 1 - 6 logₑx)/(x(e^x + e^(-x))) dx is:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Options:**\n\nA. logₑ2\nB. 2\nC. 1\nD. e/2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---\n\n### Model Response\n\nsystem\nThink step-by-step; put only the final answer inside \\boxed{}.\nuser\nThe value of ∫(logₑx + 1 - 6 logₑx)/(x(e^x + e^(-x))) dx is:\nassistant\nThe value of ∫(logₑx + 1 - 6 logₑx)/(x(e^x + e^(-x))) dx is: 1/2 logₑ(e^x + e^(-x)) + C"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for files in json_files:\n",
        "  for i in range(25):\n",
        "    ex_ans, ans = extract_boxed_answer_from_json(files, i)\n",
        "\n",
        "    print(\"Extracted answer:\", ex_ans, \"real_answer: \", ans)\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ5UTVl9bZHv",
        "outputId": "7944f50e-ee3d-478b-890f-af6dc1bb7834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  5\n",
            "Extracted answer: e^2 - 1 real_answer:  e² – 1\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  22\n",
            "Extracted answer: 1 real_answer:  29\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  22π²\n",
            "Extracted answer: \\frac{1}{6} real_answer:  48\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  784\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  (–14/3, 22/3, 3)\n",
            "Extracted answer: 1 real_answer:  e^(8/5)\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  2/3\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  5148\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  3 – e\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  6\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  15\n",
            "Extracted answer: 1 real_answer:  1\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  2525\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  31\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  6π – 16\n",
            "Extracted answer: 14 real_answer:  14\n",
            "Extracted answer: \\frac{288}{5} real_answer:  288/5\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  34\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  2035\n",
            "Extracted answer: 1/2I - A/2 + A^2/2 real_answer:  34\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  216\n",
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_response_by_index(json_files[0], 17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "a9WqHd5zbZJk",
        "outputId": "60c9cf43-ae9f-481d-ad1d-8cdf6e89ba1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question 18\n\n**Q:** The area of the region, inside the circle (2x – 2)² + (2y – 3)² = 12 and outside the parabola y² = 2√3x is:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Options:**\n\nA. 6π – 8\nB. 3π – 8\nC. 6π – 16\nD. 3π + 8"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---\n\n### Model Response\n\nsystem\nThink step-by-step; put only the final answer inside \\boxed{}.\nuser\nThe area of the region, inside the circle (2x – 2)² + (2y – 3)² = 12 and outside the parabola y² = 2√3x is:\nassistant\nThe area of the region, inside the circle (2x – 2)² + (2y – 3)² = 12 and outside the parabola y² = 2√3x is: 12π - 6√3"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_extracted_answers_to_json_files(file_paths, key=\"qwen-2.5-math_response_Wquestion_cleaned_ar\"):\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for i in range(len(data)):\n",
        "                extracted, _ = extract_boxed_answer_from_json(file_path, i, key=key)\n",
        "                data[i][\"extract_ans_from_boxed\"] = extracted\n",
        "\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                f.flush()\n",
        "                os.fsync(f.fileno())\n",
        "\n",
        "            print(f\"✅ Updated: {file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "-V6yTn4cfGNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_extracted_answers_to_json_files(json_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7fVzHZtfGSS",
        "outputId": "377abfbf-a461-4e56-e702-c515cb1d2bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "No \\boxed{} answer found in the response.\n",
            "✅ Updated: /content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_missing_boxed_answers(file_paths, verbose=False):\n",
        "    total_missing = 0\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for i, obj in enumerate(data):\n",
        "                if obj.get(\"extract_ans_from_boxed\") is None:\n",
        "                    total_missing += 1\n",
        "                    if verbose:\n",
        "                        print(f\"❌ Missing at → File: {os.path.basename(file_path)}, Index: {i}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error reading {file_path}: {e}\")\n",
        "\n",
        "    return total_missing\n"
      ],
      "metadata": {
        "id": "HNdnt37xf3eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count silently\n",
        "missing_total = count_missing_boxed_answers(json_files, verbose=True)\n",
        "print(\"Total missing boxed answers:\", missing_total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrX5vr5SfGTk",
        "outputId": "e279e03d-eca8-4693-8e27-3da89431a900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 0\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 2\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 4\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 6\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 7\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 9\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 10\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 11\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 12\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 13\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 15\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 16\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 17\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 20\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 21\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 23\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_1_questions.json, Index: 24\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 0\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 1\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 3\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 4\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 8\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 10\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 12\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 14\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 15\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 16\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 17\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 18\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_1_questions.json, Index: 22\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 0\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 1\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 2\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 4\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 5\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 6\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 7\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 9\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 10\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 11\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 12\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 14\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 15\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 17\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 19\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_22_shift_2_questions.json, Index: 21\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 0\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 1\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 5\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 7\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 8\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 9\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 10\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 11\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 12\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 13\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 14\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 15\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 16\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 17\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 18\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 21\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 22\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 23\n",
            "❌ Missing at → File: pred_only_question_20250731_094406_jan_23_shift_2_questions.json, Index: 24\n",
            "Total missing boxed answers: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_files[2])\n",
        "show_response_by_index(json_files[2], 22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "kTvE4jq-fGVg",
        "outputId": "e89cacda-a07e-49ed-fc31-28cdb7e67fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aryabhata_Eval/Jee-questions-maths-2025/qwen-2.5-math/only_question_20250731_094406/pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question 23\n\n**Q:** Let the distance between two parallel lines be 5 units and a point P lie between the lines at a unit distance from one of them. An equilateral triangle PQR is formed such that Q lies on one line and R on the other. Then (QR)² is equal to:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---\n\n### Model Response\n\nsystem\nThink step-by-step; put only the final answer inside \\boxed{}.\nuser\nLet the distance between two parallel lines be 5 units and a point P lie between the lines at a unit distance from one of them. An equilateral triangle PQR is formed such that Q lies on one line and R on the other. Then (QR)² is equal to:\nassistant\nLet the two parallel lines be \\( L_1 \\) and \\( L_2 \\), and let the distance between them be 5 units. Let point \\( P \\) lie between the lines such that the distance from \\( P \\) to \\( L_1 \\) is 1 unit. Therefore, the distance from \\( P \\) to \\( L_2 \\) is \\( 5 - 1 = 4 \\) units.\n\nWe need to form an equilateral triangle \\( PQR \\) such that \\( Q \\) lies on \\( L_1 \\) and \\( R \\) lies on \\( L_2 \\). Let the side length of the equilateral triangle be \\( s \\). The height of an equilateral triangle with side length \\( s \\) is given by \\( \\frac{\\sqrt{3}}{2} s \\).\n\nSince the height of the equilateral triangle is the sum of the distances from \\( P \\) to \\( L_1 \\) and from \\( P \\) to \\( L_2 \\), we have:\n\\[\n\\frac{\\sqrt{3}}{2} s = 1 + 4 = 5\n\\]\nSolving for \\( s \\), we get:\n\\[\ns = \\frac{10}{\\sqrt{3}} = \\frac{10\\sqrt{3}}{3}\n\\]\nThe square of the side length \\( s \\) is:\n\\[\ns^2 = \\left( \\frac{10\\sqrt{3}}{3} \\right)^2 = \\frac{100 \\cdot 3}{9} = \\frac{300}{9} = \\frac{100}{3}\n\\]\nThus, the value of \\( (QR)^2 \\) is:\n\\[\n\\boxed{\\frac{100}{3}}\n\\]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert extracted answers into base form"
      ],
      "metadata": {
        "id": "XjvBwVKIgkLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "def normalize_latex_expression(expr: str):\n",
        "    try:\n",
        "        if not expr:\n",
        "            return None\n",
        "\n",
        "        expr = expr.replace(\"\\\\\\\\\", \"\\\\\")\n",
        "        expr = expr.replace(r\"\\pi\", \"π\").replace(r\"\\infty\", \"∞\")\n",
        "        expr = expr.replace(\"–\", \"-\").replace(\"−\", \"-\")\n",
        "        expr = expr.strip()\n",
        "\n",
        "        # Handle \\frac\n",
        "        expr = re.sub(r'\\\\frac\\s*{([^{}]+)}{([^{}]+)}', r'(\\1)/(\\2)', expr)\n",
        "        expr = re.sub(r'\\\\boxed{(.*?)}', r'\\1', expr)\n",
        "\n",
        "        # Check for interval (e.g. (-oo, oo))\n",
        "        interval_match = re.match(r'\\(?-?oo\\s*,\\s*oo\\)?', expr)\n",
        "        if interval_match:\n",
        "            return \"(-∞, ∞)\"  # standardized symbolic string\n",
        "\n",
        "        # Try parsing as equation\n",
        "        if \"=\" in expr:\n",
        "            lhs, rhs = expr.split(\"=\")\n",
        "            return sp.Eq(sp.sympify(lhs.strip()), sp.sympify(rhs.strip()))\n",
        "\n",
        "        # Else treat as regular expression\n",
        "        return sp.sympify(expr, evaluate=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "SpUa-u1klihq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_latex_expression(r\"\\frac{1}{3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "kbpfvv0Hjg6z",
        "outputId": "bd876f6d-bbce-4d1e-c75b-e1a50e7c089b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1/3"
            ],
            "text/latex": "$\\displaystyle \\frac{1}{3}$"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalized_answer(json_path, index, key=\"extract_ans_from_boxed\"):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if index < 0 or index >= len(data):\n",
        "            print(f\"Index {index} out of range.\")\n",
        "            return None\n",
        "\n",
        "        raw_ans = data[index].get(key)\n",
        "        # print(normalize_latex_expression(raw_ans), raw_ans)\n",
        "        if type(raw_ans) == list:\n",
        "            raw_ans = raw_ans[0]\n",
        "        if not raw_ans:\n",
        "            print(f\"No value found at index {index} for key '{key}'\")\n",
        "            return None\n",
        "\n",
        "        normalized = normalize_latex_expression(raw_ans)\n",
        "        return normalized\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing {json_path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "EjROk1P3iiaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_ans, ans = extract_boxed_answer_from_json(json_files[0], 10)\n",
        "print(\"Extracted answer:\", ex_ans, \"real_answer: \", ans)\n",
        "\n",
        "normalized = get_normalized_answer(json_files[0], 10)\n",
        "print(\"Normalized:\", normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnisz-yCiidI",
        "outputId": "e5da85de-224b-428c-8fd2-1496bf2fadcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "Extracted answer: None real_answer:  5148\n",
            "No value found at index 10 for key 'extract_ans_from_boxed'\n",
            "Normalized: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for files in json_files:\n",
        "  for i in range(25):\n",
        "    ex_ans, ans = extract_boxed_answer_from_json(files, i)\n",
        "\n",
        "    print()\n",
        "\n",
        "    normalized = get_normalized_answer(files, i)\n",
        "    print(\"Extracted answer:\", ex_ans, \"Normalized:\", normalized, \"real_answer: \", ans)\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGBIY7o_iifW",
        "outputId": "5034ad48-4adf-4c0a-b615-c6b84be6bd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 0 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  5\n",
            "\n",
            "Extracted answer: e^2 - 1 Normalized: e**2 - 1 real_answer:  e² – 1\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 2 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  22\n",
            "\n",
            "Extracted answer: 1 Normalized: 1 real_answer:  29\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 4 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  22π²\n",
            "\n",
            "Extracted answer: \\frac{1}{6} Normalized: 1/6 real_answer:  48\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 6 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  784\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 7 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  (–14/3, 22/3, 3)\n",
            "\n",
            "Extracted answer: 1 Normalized: 1 real_answer:  e^(8/5)\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 9 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  2/3\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 10 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  5148\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 11 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  3 – e\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 12 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  6\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 13 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  15\n",
            "\n",
            "Extracted answer: 1 Normalized: 1 real_answer:  1\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 15 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  2525\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 16 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  31\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 17 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  6π – 16\n",
            "\n",
            "Extracted answer: 14 Normalized: 14 real_answer:  14\n",
            "\n",
            "Extracted answer: \\frac{288}{5} Normalized: 288/5 real_answer:  288/5\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 20 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  34\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 21 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  2035\n",
            "\n",
            "Extracted answer: 1/2I - A/2 + A^2/2 Normalized: None real_answer:  34\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 23 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  216\n",
            "No \\boxed{} answer found in the response.\n",
            "\n",
            "No value found at index 24 for key 'extract_ans_from_boxed'\n",
            "Extracted answer: None Normalized: None real_answer:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def mark_numerical_vs_special_answers(json_path, input_key=\"extract_ans_from_boxed\", output_key=\"check_ex_answer\"):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for obj in data:\n",
        "            ans = obj.get(input_key)\n",
        "\n",
        "            if type(ans) == list:\n",
        "                ans = ans[0]\n",
        "\n",
        "\n",
        "            if ans is None:\n",
        "                obj[output_key] = None\n",
        "            else:\n",
        "                try:\n",
        "                    obj[output_key] = float(ans)\n",
        "                except:\n",
        "                    obj[output_key] = \"SPECI\"\n",
        "\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno())\n",
        "\n",
        "        print(f\"✅ Updated file: {os.path.basename(json_path)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {json_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "uwZ3xVR1iih3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in json_files:\n",
        "  mark_numerical_vs_special_answers(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHJQltDAnKQj",
        "outputId": "b737bb88-e1de-46f4-ffb0-42694a180247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a numerical version of correct answers\n",
        "\n",
        "for files in json_files:\n",
        "  mark_numerical_vs_special_answers(files, input_key=\"answer\", output_key=\"converted_answers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7ShP_dNnKVm",
        "outputId": "2b4f2659-9918-4709-8bc5-1ff9412f2b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "✅ Updated file: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### check Float matching to the numerical answers questions"
      ],
      "metadata": {
        "id": "C_-2SA_Ypv01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "def evaluate_matches_by_tolerance(\n",
        "    json_path,\n",
        "    pred_key=\"check_ex_answer\",\n",
        "    gold_key=\"converted_answers\",\n",
        "    output_key=\"Matched\",\n",
        "    tolerance=1e-9\n",
        "):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for obj in data:\n",
        "            pred = obj.get(pred_key)\n",
        "            gold = obj.get(gold_key)\n",
        "\n",
        "            if pred in [None, \"SPECI\"] or gold is None:\n",
        "                obj[output_key] = None\n",
        "            elif isinstance(pred, (int, float)) and isinstance(gold, (int, float)):\n",
        "                obj[output_key] = math.isclose(pred, gold, abs_tol=tolerance)\n",
        "            else:\n",
        "                obj[output_key] = None  # Non-numeric fallback\n",
        "\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno())\n",
        "\n",
        "        print(f\"✅ Match evaluation updated: {os.path.basename(json_path)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {json_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "LRKp7R-YnKXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in json_files:\n",
        "  evaluate_matches_by_tolerance(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OuwqEZYnKb9",
        "outputId": "dce6e621-dc93-4499-d4c2-ca6803924f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Match evaluation updated: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "✅ Match evaluation updated: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "✅ Match evaluation updated: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "✅ Match evaluation updated: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate total percentage of pass and failure for numerical based answers"
      ],
      "metadata": {
        "id": "2BtpJmwyqDOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def summarize_match_results(json_path, match_key=\"Matched\", verbose=False):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        total_eval = 0\n",
        "        total_correct = 0\n",
        "        total_incorrect = 0\n",
        "\n",
        "        for obj in data:\n",
        "            match = obj.get(match_key)\n",
        "            if match is True:\n",
        "                total_eval += 1\n",
        "                total_correct += 1\n",
        "            elif match is False:\n",
        "                total_eval += 1\n",
        "                total_incorrect += 1\n",
        "        if verbose:\n",
        "\n",
        "            print(f\"📊 Evaluation Summary for: {os.path.basename(json_path)}\")\n",
        "            print(f\"Total evaluated (numeric): {total_eval}\")\n",
        "            print(f\"✅ Correct matches: {total_correct} ({(total_correct/total_eval)*100:.2f}%)\")\n",
        "            print(f\"❌ Incorrect matches: {total_incorrect} ({(total_incorrect/total_eval)*100:.2f}%)\")\n",
        "\n",
        "        return {\n",
        "            \"file\": os.path.basename(json_path),\n",
        "            \"total_evaluated\": total_eval,\n",
        "            \"correct\": total_correct,\n",
        "            \"incorrect\": total_incorrect,\n",
        "            \"correct_pct\": round((total_correct / total_eval) * 100, 2) if total_eval else 0,\n",
        "            \"incorrect_pct\": round((total_incorrect / total_eval) * 100, 2) if total_eval else 0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {json_path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "qEsu8dilqIrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in json_files:\n",
        "  summarize_match_results(files, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnT54aCVqIxT",
        "outputId": "d884ed6f-f268-4a7b-f164-4326ba35188a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "Total evaluated (numeric): 3\n",
            "✅ Correct matches: 2 (66.67%)\n",
            "❌ Incorrect matches: 1 (33.33%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "Total evaluated (numeric): 5\n",
            "✅ Correct matches: 0 (0.00%)\n",
            "❌ Incorrect matches: 5 (100.00%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "Total evaluated (numeric): 4\n",
            "✅ Correct matches: 1 (25.00%)\n",
            "❌ Incorrect matches: 3 (75.00%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n",
            "Total evaluated (numeric): 2\n",
            "✅ Correct matches: 0 (0.00%)\n",
            "❌ Incorrect matches: 2 (100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Step 1: Define the output schema\n",
        "class LLMEvaluation(BaseModel):\n",
        "    llm_ex_answer: Optional[str] = Field(description=\"Extracted answer from the generated response, or None if not found.\")\n",
        "    llm_matched: bool = Field(description=\"True if the extracted answer is equivalent to the correct answer.\")\n",
        "    llm_cant_say: bool = Field(description=\"True if model cannot confidently determine equivalence without deeper derivation.\")\n",
        "    llm_reasoning: str = Field(description=\"Reasoning behind the equivalence decision.\")\n",
        "\n",
        "# Step 2: Prompt template\n",
        "parser = PydanticOutputParser(pydantic_object=LLMEvaluation)\n"
      ],
      "metadata": {
        "id": "NKYCrDKvJYVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are evaluating a mathematical question and the answer generated by a model.\n",
        "\n",
        "- Extract the final answer from the model's response (if any).\n",
        "- Compare it with the correct answer.\n",
        "- You should say they are equivalent if the match is trivially clear, such as:, Format/syntax differences (e.g., \\\\frac{{1}}{{2}} vs 1/2) or Equivalent constants or cleanly reducible numeric expressions\n",
        "- If deeper derivation is required to determine equivalence, mark `llm_cant_say = True` and do not match.\n",
        "- If no valid answer can be extracted, set `llm_ex_answer = None`.\n",
        "\n",
        "❌ Do NOT try to prove symbolic or algebraic equivalence (e.g., trigonometric identities, matrix forms, adjoints, complex fractions).\n",
        "If it requires derivation, simplification, or symbolic transformation, **mark it as requiring complex reasoning.** under llm_reasoning.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Model Response:\n",
        "{response}\n",
        "\n",
        "Correct Answer:\n",
        "{actual}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9yGvAkitKXzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Function with retry and rate limit handling\n",
        "def judge_speci_cases_with_llm(file_paths, llm=None, model=\"gemini-2.0-flash\", gemini_api_key=\"\"):\n",
        "    if llm is None:\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=model,\n",
        "            temperature=0,\n",
        "            google_api_key=gemini_api_key\n",
        "        )\n",
        "\n",
        "    for path in file_paths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            updated = False\n",
        "\n",
        "            for i, obj in enumerate(tqdm(data, desc=f\"Processing {Path(path).name}\")):\n",
        "                if \"llm_matched\" in obj:\n",
        "                    print(\"\\nskipped.\")\n",
        "                    continue\n",
        "\n",
        "                question = obj.get(\"question\", \"\")\n",
        "                actual = obj.get(\"answer\", \"\")\n",
        "                response = obj.get(\"qwen-2.5-math_response_Wquestion\", \"\")\n",
        "\n",
        "                prompt = prompt_template.format(\n",
        "                    question=question,\n",
        "                    response=response,\n",
        "                    actual=actual,\n",
        "                    format_instructions=parser.get_format_instructions()\n",
        "                )\n",
        "\n",
        "                retry_count = 0\n",
        "                max_retries = 5\n",
        "                success = False\n",
        "\n",
        "                while retry_count < max_retries:\n",
        "                    try:\n",
        "                        result = llm.invoke(prompt)\n",
        "                        parsed = parser.parse(result.content)\n",
        "\n",
        "                        obj[\"llm_ex_answer\"] = parsed.llm_ex_answer\n",
        "                        obj[\"llm_matched\"] = parsed.llm_matched\n",
        "                        obj[\"llm_cant_say\"] = parsed.llm_cant_say\n",
        "                        obj[\"llm_reasoning\"] = parsed.llm_reasoning\n",
        "\n",
        "                        updated = True\n",
        "                        success = True\n",
        "                        break  # Exit retry loop on success\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_str = str(e).lower()\n",
        "                        if \"rate limit\" in error_str or \"429\" in error_str:\n",
        "                            retry_count += 1\n",
        "                            print(f\"⚠️ Rate limit hit (retry {retry_count}/{max_retries})... waiting 60s\")\n",
        "                            time.sleep(60)\n",
        "                        else:\n",
        "                            print(f\"❌ LLM parse error at index {i}: {e}\")\n",
        "                            obj[\"llm_ex_answer\"] = None\n",
        "                            obj[\"llm_matched\"] = None\n",
        "                            obj[\"llm_cant_say\"] = None\n",
        "                            obj[\"llm_reasoning\"] = \"Parsing failed or ambiguous output.\"\n",
        "                            break  # Don't retry on non-rate-limit errors\n",
        "\n",
        "                if not success and retry_count == max_retries:\n",
        "                    print(f\"❌ Exceeded rate limits at index {i} in file {Path(path).name}. Terminating.\")\n",
        "                    return  # Exit completely if rate limit persists\n",
        "\n",
        "            if updated:\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                    f.flush()\n",
        "                    os.fsync(f.fileno())\n",
        "                print(f\"✅ Updated file: {Path(path).name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process file {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "1_WdmVGqKV3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = \"\"\n",
        "\n",
        "judge_speci_cases_with_llm([\"/content/test.json\"], model=\"gemini-2.0-flash\", gemini_api_key=gemini_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6gwo41rK1z3",
        "outputId": "3d766d68-b554-46c0-f2b4-ad3adbbc0896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test.json: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: test.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "judge_speci_cases_with_llm([json_files[0]], model=\"gemini-2.0-flash\", gemini_api_key=gemini_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmbB0Ii8Nhnq",
        "outputId": "ca4b4236-c97e-4d98-acc6-57ff6797f3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pred_only_question_20250731_094406_jan_22_shift_1_questions.json:   0%|          | 0/25 [00:00<?, ?it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 53\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5)... waiting 60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_1_questions.json:  96%|█████████▌| 24/25 [01:24<00:00,  1.10it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 29\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5)... waiting 60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_1_questions.json: 100%|██████████| 25/25 [02:28<00:00,  5.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "judge_speci_cases_with_llm([json_files[1]], model=\"gemini-2.0-flash\", gemini_api_key=gemini_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyYIUbv5K2G5",
        "outputId": "62c1cfe3-385b-4f8b-e3d1-22fe6c8fca01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_23_shift_1_questions.json:  64%|██████▍   | 16/25 [00:16<00:09,  1.02s/it]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 18\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5)... waiting 60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_23_shift_1_questions.json: 100%|██████████| 25/25 [01:27<00:00,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "judge_speci_cases_with_llm([json_files[2]], model=\"gemini-2.0-flash\", gemini_api_key=gemini_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgq5nII9REQ3",
        "outputId": "3fb079d2-7011-44f8-8966-397248a4e0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_2_questions.json:  32%|███▏      | 8/25 [00:08<00:16,  1.04it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 2\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5)... waiting 60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_2_questions.json:  96%|█████████▌| 24/25 [01:27<00:00,  1.00it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 15\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 42\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Rate limit hit (retry 1/5)... waiting 60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_2_questions.json: 100%|██████████| 25/25 [02:30<00:00,  6.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated file: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "judge_speci_cases_with_llm([json_files[3]], model=\"gemini-2.0-flash\", gemini_api_key=gemini_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCxvu8auRrdu",
        "outputId": "dc8694f6-7f11-41d9-8734-9b9517988589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_23_shift_2_questions.json: 100%|██████████| 25/25 [00:00<00:00, 90707.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n",
            "\n",
            "skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_unsolved_questions(file_paths, verbose=False):\n",
        "    total_missing = 0\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for i, obj in enumerate(data):\n",
        "                if obj.get(\"llm_ex_answer\") is None:\n",
        "                    total_missing += 1\n",
        "                    if verbose:\n",
        "                        print(f\"❌ Missing at → File: {os.path.basename(file_path)}, Index: {i}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error reading {file_path}: {e}\")\n",
        "\n",
        "    return total_missing\n"
      ],
      "metadata": {
        "id": "zisAQ4W7TrJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_unsolved_questions(json_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT2oJ9ybTrNH",
        "outputId": "8b9ba5db-eebd-4635-c597-b9dc3ae8285b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def summarize_match_results(json_path, match_key=\"Matched\", skip_key=\"llm_ex_answer\", verbose=False):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        total_eval = 0\n",
        "        total_correct = 0\n",
        "        total_incorrect = 0\n",
        "\n",
        "        for obj in data:\n",
        "            match = obj.get(match_key)\n",
        "            skip = obj.get(skip_key)\n",
        "            if match is True and skip is not None:\n",
        "                total_eval += 1\n",
        "                total_correct += 1\n",
        "            elif match is False and skip is not None:\n",
        "                total_eval += 1\n",
        "                total_incorrect += 1\n",
        "        if verbose:\n",
        "\n",
        "            print(f\"📊 Evaluation Summary for: {os.path.basename(json_path)}\")\n",
        "            print(f\"Total evaluated (numeric): {total_eval}\")\n",
        "            print(f\"✅ Correct matches: {total_correct} ({(total_correct/total_eval)*100:.2f}%)\")\n",
        "            print(f\"❌ Incorrect matches: {total_incorrect} ({(total_incorrect/total_eval)*100:.2f}%)\")\n",
        "\n",
        "        return {\n",
        "            \"file\": os.path.basename(json_path),\n",
        "            \"total_evaluated\": total_eval,\n",
        "            \"correct\": total_correct,\n",
        "            \"incorrect\": total_incorrect,\n",
        "            \"correct_pct\": round((total_correct / total_eval) * 100, 2) if total_eval else 0,\n",
        "            \"incorrect_pct\": round((total_incorrect / total_eval) * 100, 2) if total_eval else 0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {json_path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "SaU8Vd8NVLx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path in json_files:\n",
        "  result = summarize_match_results(path, match_key=\"llm_matched\", verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BYBmW4WREWj",
        "outputId": "cc7dbfe1-16b9-451c-f8e9-14cd67abcbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n",
            "Total evaluated (numeric): 19\n",
            "✅ Correct matches: 5 (26.32%)\n",
            "❌ Incorrect matches: 14 (73.68%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n",
            "Total evaluated (numeric): 17\n",
            "✅ Correct matches: 0 (0.00%)\n",
            "❌ Incorrect matches: 17 (100.00%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n",
            "Total evaluated (numeric): 17\n",
            "✅ Correct matches: 1 (5.88%)\n",
            "❌ Incorrect matches: 16 (94.12%)\n",
            "📊 Evaluation Summary for: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n",
            "Total evaluated (numeric): 13\n",
            "✅ Correct matches: 3 (23.08%)\n",
            "❌ Incorrect matches: 10 (76.92%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_multiple_files_match_results(file_paths, match_key=\"Matched\"):\n",
        "    grand_total = {\n",
        "        \"total_evaluated\": 0,\n",
        "        \"correct\": 0,\n",
        "        \"incorrect\": 0\n",
        "    }\n",
        "\n",
        "    print(\"📊 Summary per file:\\n\" + \"-\" * 35)\n",
        "    for path in file_paths:\n",
        "        result = summarize_match_results(path, match_key)\n",
        "        if result:\n",
        "            grand_total[\"total_evaluated\"] += result[\"total_evaluated\"]\n",
        "            grand_total[\"correct\"] += result[\"correct\"]\n",
        "            grand_total[\"incorrect\"] += result[\"incorrect\"]\n",
        "\n",
        "    total = grand_total[\"total_evaluated\"]\n",
        "    correct = grand_total[\"correct\"]\n",
        "    incorrect = grand_total[\"incorrect\"]\n",
        "\n",
        "    print(\"\\n🧾 Grand Total Summary\\n\" + \"=\" * 35)\n",
        "    print(f\"Total evaluated: {total}\")\n",
        "    print(f\"✅ Correct: {correct} ({(correct/total)*100:.2f}%)\")\n",
        "    print(f\"❌ Incorrect: {incorrect} ({(incorrect/total)*100:.2f}%)\")\n",
        "\n",
        "    return {\n",
        "        \"total_evaluated\": total,\n",
        "        \"correct\": correct,\n",
        "        \"incorrect\": incorrect,\n",
        "        \"correct_pct\": round((correct / total) * 100, 2) if total else 0,\n",
        "        \"incorrect_pct\": round((incorrect / total) * 100, 2) if total else 0\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2gY_N_syqIzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_multiple_files_match_results(json_files, match_key=\"llm_matched\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykt3UX2xqI1M",
        "outputId": "715235bb-f2f7-4cff-852a-fd3b06de29f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Summary per file:\n",
            "-----------------------------------\n",
            "\n",
            "🧾 Grand Total Summary\n",
            "===================================\n",
            "Total evaluated: 66\n",
            "✅ Correct: 9 (13.64%)\n",
            "❌ Incorrect: 57 (86.36%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_evaluated': 66,\n",
              " 'correct': 9,\n",
              " 'incorrect': 57,\n",
              " 'correct_pct': 13.64,\n",
              " 'incorrect_pct': 86.36}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "\n",
        "# Define the system prompt used in your template\n",
        "SYSTEM_PROMPT = \"Think step-by-step; put only the final answer inside \\\\boxed{}.\"\n",
        "\n",
        "# Initialize tiktoken encoding (we'll assume GPT-3.5 / GPT-4 tokenizer for now)\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def add_token_counts_to_json_tiktoken(json_paths: list[str]):\n",
        "    for path in json_paths:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            updated = False\n",
        "            for obj in tqdm(data, desc=f\"Processing {os.path.basename(path)}\"):\n",
        "                question = obj.get(\"question\", \"\")\n",
        "                response = obj.get(\"qwen-2.5-math_response_Wquestion\", \"\")\n",
        "\n",
        "                # Prompt format with system message\n",
        "                prompt = (\n",
        "                    f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
        "                    f\"<|im_start|>user\\n{question}<|im_end|>\"\n",
        "                )\n",
        "\n",
        "                # Token counts\n",
        "                input_token_count = count_tokens(question)\n",
        "                prompt_token_count = count_tokens(prompt)\n",
        "                output_token_count = count_tokens(response)\n",
        "                total_token_count = prompt_token_count + output_token_count\n",
        "\n",
        "                obj[\"input_token_count\"] = input_token_count\n",
        "                obj[\"prompt_token_count\"] = prompt_token_count\n",
        "                obj[\"output_token_count\"] = output_token_count\n",
        "                obj[\"total_token_count\"] = total_token_count\n",
        "\n",
        "                updated = True\n",
        "\n",
        "            if updated:\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"✅ Token counts added to: {os.path.basename(path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "91ZPEUqnhgTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_token_counts_to_json_tiktoken(json_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4OlB0URhhNA",
        "outputId": "855228f1-c3d0-405e-c226-5ca32b658da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_1_questions.json: 100%|██████████| 25/25 [00:00<00:00, 1018.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: pred_only_question_20250731_094406_jan_22_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_23_shift_1_questions.json: 100%|██████████| 25/25 [00:00<00:00, 678.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: pred_only_question_20250731_094406_jan_23_shift_1_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_22_shift_2_questions.json: 100%|██████████| 25/25 [00:00<00:00, 967.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: pred_only_question_20250731_094406_jan_22_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pred_only_question_20250731_094406_jan_23_shift_2_questions.json: 100%|██████████| 25/25 [00:00<00:00, 987.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Token counts added to: pred_only_question_20250731_094406_jan_23_shift_2_questions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Results"
      ],
      "metadata": {
        "id": "bchz6z-LbfHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extended_summary_per_file(json_files):\n",
        "    file_results = []\n",
        "\n",
        "    for path in json_files:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        total_questions = len(data)\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        incorrect = 0\n",
        "        not_evaluated = 0\n",
        "        llm_judge_count = 0\n",
        "\n",
        "        input_tokens, output_tokens, prompt_tokens = [], [], []\n",
        "\n",
        "        for obj in data:\n",
        "            matched = obj.get(\"llm_matched\")\n",
        "            check_ex = obj.get(\"llm_ex_answer\")\n",
        "\n",
        "            # Token counts\n",
        "            if obj.get(\"input_token_count\") is not None:\n",
        "                input_tokens.append(obj[\"input_token_count\"])\n",
        "            if obj.get(\"output_token_count\") is not None:\n",
        "                output_tokens.append(obj[\"output_token_count\"])\n",
        "            if obj.get(\"prompt_token_count\") is not None:\n",
        "                prompt_tokens.append(obj[\"prompt_token_count\"])\n",
        "\n",
        "            if check_ex is None:\n",
        "                not_evaluated += 1\n",
        "                continue\n",
        "\n",
        "            if matched is True:\n",
        "                correct += 1\n",
        "            elif matched is False:\n",
        "                incorrect += 1\n",
        "            else:\n",
        "                not_evaluated += 1\n",
        "\n",
        "            if obj.get(\"llm_cant_say\") is False and obj.get(\"llm_ex_answer\") is not None:\n",
        "                llm_judge_count += 1\n",
        "\n",
        "        total = correct + incorrect\n",
        "\n",
        "        def token_stats(lst):\n",
        "            return {\n",
        "                \"avg\": round(sum(lst) / len(lst), 2) if lst else 0,\n",
        "                \"min\": min(lst) if lst else 0,\n",
        "                \"max\": max(lst) if lst else 0\n",
        "            }\n",
        "\n",
        "        input_stats = token_stats(input_tokens)\n",
        "        output_stats = token_stats(output_tokens)\n",
        "        prompt_stats = token_stats(prompt_tokens)\n",
        "\n",
        "        file_results.append({\n",
        "            \"file\": Path(path).name,\n",
        "            \"total_questions\": total_questions,\n",
        "            \"evaluated\": total,\n",
        "            \"correct\": correct,\n",
        "            \"incorrect\": incorrect,\n",
        "            \"not_evaluated\": not_evaluated,\n",
        "            \"% evaluated\": round((total / total_questions) * 100, 2) if total_questions else 0,\n",
        "            \"llm_judged\": llm_judge_count,\n",
        "            \"correct_pct\": round((correct / total) * 100, 2) if total else 0,\n",
        "            \"incorrect_pct\": round((incorrect / total) * 100, 2) if total else 0,\n",
        "\n",
        "            # Token stats\n",
        "            \"input_avg_tokens\": input_stats[\"avg\"],\n",
        "            \"input_min_tokens\": input_stats[\"min\"],\n",
        "            \"input_max_tokens\": input_stats[\"max\"],\n",
        "\n",
        "            \"output_avg_tokens\": output_stats[\"avg\"],\n",
        "            \"output_min_tokens\": output_stats[\"min\"],\n",
        "            \"output_max_tokens\": output_stats[\"max\"],\n",
        "\n",
        "            \"prompt_avg_tokens\": prompt_stats[\"avg\"],\n",
        "            \"prompt_min_tokens\": prompt_stats[\"min\"],\n",
        "            \"prompt_max_tokens\": prompt_stats[\"max\"],\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(file_results)\n",
        "\n",
        "\n",
        "def grand_total_extended_summary(df: pd.DataFrame):\n",
        "    total = df[\"evaluated\"].sum()\n",
        "    correct = df[\"correct\"].sum()\n",
        "    incorrect = df[\"incorrect\"].sum()\n",
        "    not_evaluated = df[\"not_evaluated\"].sum()\n",
        "    llm_judged_total = df[\"llm_judged\"].sum()\n",
        "\n",
        "    summary = {\n",
        "        \"total_evaluated\": total,\n",
        "        \"correct\": correct,\n",
        "        \"incorrect\": incorrect,\n",
        "        \"not_evaluated\": not_evaluated,\n",
        "        \"llm_judged\": llm_judged_total,\n",
        "        \"correct_pct\": round((correct / total) * 100, 2) if total else 0,\n",
        "        \"incorrect_pct\": round((incorrect / total) * 100, 2) if total else 0,\n",
        "\n",
        "        # Token stats from all files\n",
        "        \"input_avg_tokens\": round(df[\"input_avg_tokens\"].mean(), 2),\n",
        "        \"input_min_tokens\": df[\"input_min_tokens\"].min(),\n",
        "        \"input_max_tokens\": df[\"input_max_tokens\"].max(),\n",
        "\n",
        "        \"output_avg_tokens\": round(df[\"output_avg_tokens\"].mean(), 2),\n",
        "        \"output_min_tokens\": df[\"output_min_tokens\"].min(),\n",
        "        \"output_max_tokens\": df[\"output_max_tokens\"].max(),\n",
        "\n",
        "        \"prompt_avg_tokens\": round(df[\"prompt_avg_tokens\"].mean(), 2),\n",
        "        \"prompt_min_tokens\": df[\"prompt_min_tokens\"].min(),\n",
        "        \"prompt_max_tokens\": df[\"prompt_max_tokens\"].max(),\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame([summary])\n"
      ],
      "metadata": {
        "id": "rvy5jynbi-U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = extended_summary_per_file(json_files)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "hqjWj3fbjtwD",
        "outputId": "3dbb1d0b-e87d-42b7-d79e-322beefdb1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                file  total_questions  \\\n",
              "0  pred_only_question_20250731_094406_jan_22_shif...               25   \n",
              "1  pred_only_question_20250731_094406_jan_23_shif...               25   \n",
              "2  pred_only_question_20250731_094406_jan_22_shif...               25   \n",
              "3  pred_only_question_20250731_094406_jan_23_shif...               25   \n",
              "\n",
              "   evaluated  correct  incorrect  not_evaluated  % evaluated  llm_judged  \\\n",
              "0         19        5         14              6         76.0          15   \n",
              "1         17        0         17              8         68.0          11   \n",
              "2         17        1         16              8         68.0           8   \n",
              "3         13        3         10             12         52.0          10   \n",
              "\n",
              "   correct_pct  incorrect_pct  input_avg_tokens  input_min_tokens  \\\n",
              "0        26.32          73.68             57.88                19   \n",
              "1         0.00         100.00             50.64                14   \n",
              "2         5.88          94.12             62.60                32   \n",
              "3        23.08          76.92             46.52                15   \n",
              "\n",
              "   input_max_tokens  output_avg_tokens  output_min_tokens  output_max_tokens  \\\n",
              "0               123            1170.84                 61               4234   \n",
              "1                84            1644.16                 63               4167   \n",
              "2                94            1194.32                 89               4363   \n",
              "3                87            1266.52                 46               4082   \n",
              "\n",
              "   prompt_avg_tokens  prompt_min_tokens  prompt_max_tokens  \n",
              "0              98.88                 60                164  \n",
              "1              91.84                 55                126  \n",
              "2             103.60                 73                135  \n",
              "3              87.76                 56                128  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da696f0f-aac4-4197-820b-e744051c1f8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>total_questions</th>\n",
              "      <th>evaluated</th>\n",
              "      <th>correct</th>\n",
              "      <th>incorrect</th>\n",
              "      <th>not_evaluated</th>\n",
              "      <th>% evaluated</th>\n",
              "      <th>llm_judged</th>\n",
              "      <th>correct_pct</th>\n",
              "      <th>incorrect_pct</th>\n",
              "      <th>input_avg_tokens</th>\n",
              "      <th>input_min_tokens</th>\n",
              "      <th>input_max_tokens</th>\n",
              "      <th>output_avg_tokens</th>\n",
              "      <th>output_min_tokens</th>\n",
              "      <th>output_max_tokens</th>\n",
              "      <th>prompt_avg_tokens</th>\n",
              "      <th>prompt_min_tokens</th>\n",
              "      <th>prompt_max_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pred_only_question_20250731_094406_jan_22_shif...</td>\n",
              "      <td>25</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>76.0</td>\n",
              "      <td>15</td>\n",
              "      <td>26.32</td>\n",
              "      <td>73.68</td>\n",
              "      <td>57.88</td>\n",
              "      <td>19</td>\n",
              "      <td>123</td>\n",
              "      <td>1170.84</td>\n",
              "      <td>61</td>\n",
              "      <td>4234</td>\n",
              "      <td>98.88</td>\n",
              "      <td>60</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pred_only_question_20250731_094406_jan_23_shif...</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>50.64</td>\n",
              "      <td>14</td>\n",
              "      <td>84</td>\n",
              "      <td>1644.16</td>\n",
              "      <td>63</td>\n",
              "      <td>4167</td>\n",
              "      <td>91.84</td>\n",
              "      <td>55</td>\n",
              "      <td>126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pred_only_question_20250731_094406_jan_22_shif...</td>\n",
              "      <td>25</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>8</td>\n",
              "      <td>5.88</td>\n",
              "      <td>94.12</td>\n",
              "      <td>62.60</td>\n",
              "      <td>32</td>\n",
              "      <td>94</td>\n",
              "      <td>1194.32</td>\n",
              "      <td>89</td>\n",
              "      <td>4363</td>\n",
              "      <td>103.60</td>\n",
              "      <td>73</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pred_only_question_20250731_094406_jan_23_shif...</td>\n",
              "      <td>25</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "      <td>52.0</td>\n",
              "      <td>10</td>\n",
              "      <td>23.08</td>\n",
              "      <td>76.92</td>\n",
              "      <td>46.52</td>\n",
              "      <td>15</td>\n",
              "      <td>87</td>\n",
              "      <td>1266.52</td>\n",
              "      <td>46</td>\n",
              "      <td>4082</td>\n",
              "      <td>87.76</td>\n",
              "      <td>56</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da696f0f-aac4-4197-820b-e744051c1f8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da696f0f-aac4-4197-820b-e744051c1f8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da696f0f-aac4-4197-820b-e744051c1f8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3ed05266-05d4-4e3c-9377-f706a7a42670\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ed05266-05d4-4e3c-9377-f706a7a42670')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3ed05266-05d4-4e3c-9377-f706a7a42670 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_18e4dd23-8109-40b1-9bc4-a06819c2db1f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_18e4dd23-8109-40b1-9bc4-a06819c2db1f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"pred_only_question_20250731_094406_jan_23_shift_1_questions.json\",\n          \"pred_only_question_20250731_094406_jan_23_shift_2_questions.json\",\n          \"pred_only_question_20250731_094406_jan_22_shift_1_questions.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_questions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 25,\n        \"max\": 25,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 13,\n        \"max\": 19,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 10,\n        \"max\": 17,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"not_evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 6,\n        \"max\": 12,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"% evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.066445913694333,\n        \"min\": 52.0,\n        \"max\": 76.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          76.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"llm_judged\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 8,\n        \"max\": 15,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.858636526993573,\n        \"min\": 0.0,\n        \"max\": 26.32,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.858636526993571,\n        \"min\": 73.68,\n        \"max\": 100.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.201435042175043,\n        \"min\": 46.52,\n        \"max\": 62.6,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          50.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 14,\n        \"max\": 32,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 84,\n        \"max\": 123,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 220.58985893886123,\n        \"min\": 1170.84,\n        \"max\": 1644.16,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1644.16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 46,\n        \"max\": 89,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 118,\n        \"min\": 4082,\n        \"max\": 4363,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.078982977801256,\n        \"min\": 87.76,\n        \"max\": 103.6,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          91.84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 55,\n        \"max\": 73,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 126,\n        \"max\": 164,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grand_total_extended_summary(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "Akp8G6l3jy_r",
        "outputId": "c348cd3a-c997-4ef1-eec5-0d38b6d22211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   total_evaluated  correct  incorrect  not_evaluated  llm_judged  \\\n",
              "0               66        9         57             34          44   \n",
              "\n",
              "   correct_pct  incorrect_pct  input_avg_tokens  input_min_tokens  \\\n",
              "0        13.64          86.36             54.41                14   \n",
              "\n",
              "   input_max_tokens  output_avg_tokens  output_min_tokens  output_max_tokens  \\\n",
              "0               123            1318.96                 46               4363   \n",
              "\n",
              "   prompt_avg_tokens  prompt_min_tokens  prompt_max_tokens  \n",
              "0              95.52                 55                164  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23c9bd14-fff0-4eb2-aed6-a3ef9c08beff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_evaluated</th>\n",
              "      <th>correct</th>\n",
              "      <th>incorrect</th>\n",
              "      <th>not_evaluated</th>\n",
              "      <th>llm_judged</th>\n",
              "      <th>correct_pct</th>\n",
              "      <th>incorrect_pct</th>\n",
              "      <th>input_avg_tokens</th>\n",
              "      <th>input_min_tokens</th>\n",
              "      <th>input_max_tokens</th>\n",
              "      <th>output_avg_tokens</th>\n",
              "      <th>output_min_tokens</th>\n",
              "      <th>output_max_tokens</th>\n",
              "      <th>prompt_avg_tokens</th>\n",
              "      <th>prompt_min_tokens</th>\n",
              "      <th>prompt_max_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>66</td>\n",
              "      <td>9</td>\n",
              "      <td>57</td>\n",
              "      <td>34</td>\n",
              "      <td>44</td>\n",
              "      <td>13.64</td>\n",
              "      <td>86.36</td>\n",
              "      <td>54.41</td>\n",
              "      <td>14</td>\n",
              "      <td>123</td>\n",
              "      <td>1318.96</td>\n",
              "      <td>46</td>\n",
              "      <td>4363</td>\n",
              "      <td>95.52</td>\n",
              "      <td>55</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23c9bd14-fff0-4eb2-aed6-a3ef9c08beff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23c9bd14-fff0-4eb2-aed6-a3ef9c08beff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23c9bd14-fff0-4eb2-aed6-a3ef9c08beff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"grand_total_extended_summary(df)\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"total_evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 66,\n        \"max\": 66,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 9,\n        \"max\": 9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 57,\n        \"max\": 57,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          57\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"not_evaluated\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 34,\n        \"max\": 34,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"llm_judged\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 44,\n        \"max\": 44,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 13.64,\n        \"max\": 13.64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          13.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 86.36,\n        \"max\": 86.36,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          86.36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 54.41,\n        \"max\": 54.41,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          54.41\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 14,\n        \"max\": 14,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 123,\n        \"max\": 123,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1318.96,\n        \"max\": 1318.96,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1318.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 46,\n        \"max\": 46,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4363,\n        \"max\": 4363,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_avg_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 95.52,\n        \"max\": 95.52,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          95.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_min_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 55,\n        \"max\": 55,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_max_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 164,\n        \"max\": 164,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          164\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"qwen_2.5_math_results.csv\")"
      ],
      "metadata": {
        "id": "wohH8u31kM5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grand_total_extended_summary(df).to_csv(\"qwen_2.5_math_total_results.csv\")"
      ],
      "metadata": {
        "id": "IBk4dfTfXBnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ALHAFKrXKLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}